log_reg_oos_performance <- bin_classif_eval(
log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
summary(log_reg_model)
log_reg_pred_probs <- predict(
log_reg_model, newdata=x_valid, type='prob')
log_reg_model
log_reg_pred_probs <- predict(
log_reg_model, newdata=x_valid, type='prob')
log_reg_oos_performance <- bin_classif_eval(
log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
log_reg_pred_probs <- predict(
log_reg_model, newdata=x_valid, type='prob')
log_reg_model <- train(
#  x=x_train[,1:5, with=FALSE],
x=x_train,
y=y_train,
preProcess=c('center', 'scale'),
method='plr', # Penalized Logistic Regression
metric=caret_optimized_metric,
trControl=caret_train_control,
tuneGrid=expand.grid(
lambda=0, # weight penalty parameter
cp='aic')) # complexity parameter (AIC / BIC)
og_reg_model
log_reg_model
summary(log_reg_model)
log_reg_pred_probs <- predict(
log_reg_model, newdata=x_valid, type='prob')
log_reg_oos_performance <- bin_classif_eval(
log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
plot(x=1 - rf_oos_performance$specificity,
y=rf_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - boost_oos_performance$specificity,
y=boost_oos_performance$sensitivity,
col='green', lwd=3)
#legend('right', c('Random Forest', 'Boosted Trees'),
#       lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='red', lwd=3)
legend('right', c('Random Forest', 'Boosted Trees', 'Logistic Regression'),
lty=1, col=c('darkgreen', 'green', 'red'), lwd=3, cex=1.)
View(boost_oos_performance)
summary(boost_model)
log_reg_oos_performance[i, ]
sensitivity_threshold <- .8
i <- min(which(log_reg_oos_performance$sensitivity < sensitivity_threshold)) - 1
selected_prob_threshold <- prob_thresholds[i]
log_reg_oos_performance[i, ]
source('~/.active-rstudio-document', echo=TRUE)
new_Accident <- Accident[,c(1:3,6:12,14,15,19), with=FALSE]
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
new_log_reg_model <- train(
#  x=x_train[,1:5, with=FALSE],
x=new_x_train,
y=y_train,
preProcess=c('center', 'scale'),
method='plr', # Penalized Logistic Regression
metric=caret_optimized_metric,
trControl=caret_train_control,
tuneGrid=expand.grid(
lambda=0, # weight penalty parameter
cp='aic')) # complexity parameter (AIC / BIC)
new_log_reg_pred_probs <- predict(
new_log_reg_model, newdata=new_x_valid, type='prob')
new_log_reg_oos_performance <- bin_classif_eval(
new_log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
dev.new( )
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
dev.new( )
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
par( )
plot.new()
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
new_Accident <- Accident[,c(1:3,6:12,14,15,19), with=FALSE]
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
new_log_reg_model <- train(
#  x=x_train[,1:5, with=FALSE],
x=new_x_train,
y=y_train,
preProcess=c('center', 'scale'),
method='plr', # Penalized Logistic Regression
metric=caret_optimized_metric,
trControl=caret_train_control,
tuneGrid=expand.grid(
lambda=0, # weight penalty parameter
cp='aic')) # complexity parameter (AIC / BIC)
new_log_reg_pred_probs <- predict(
new_log_reg_model, newdata=new_x_valid, type='prob')
new_log_reg_oos_performance <- bin_classif_eval(
new_log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
new_Accident <- Accident[,c(1:3,6:12,14,15,19), with=FALSE]
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
new_Accident <- Accident[,c(1:3,6:12,14,15,19), with=FALSE]
new_Accident <- as.data.table(read.csv(file.path(data_folder_path, 'Accidents.csv')))[,c(1:3,6:12,14,15,19), with=FALSE]
for(colname in names(Accident)) {
Accident[[colname]] <- as.numeric(Accident[[colname]])
}
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
View(new_Accident)
View(Accident)
new_Accident <- as.data.table(read.csv(file.path(data_folder_path, 'Accidents.csv')))[,c(1:3,6:12,14,15,19), with=FALSE]
for(colname in names(Accident)) {
Accident[[colname]] <- as.numeric(Accident[[colname]])
}
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
new_log_reg_model <- train(
#  x=x_train[,1:5, with=FALSE],
x=new_x_train,
y=y_train,
preProcess=c('center', 'scale'),
method='plr', # Penalized Logistic Regression
metric=caret_optimized_metric,
trControl=caret_train_control,
tuneGrid=expand.grid(
lambda=0, # weight penalty parameter
cp='aic')) # complexity parameter (AIC / BIC)
new_log_reg_pred_probs <- predict(
new_log_reg_model, newdata=new_x_valid, type='prob')
new_log_reg_oos_performance <- bin_classif_eval(
new_log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
plot.new()
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
new_Accident <- as.data.table(read.csv(file.path(data_folder_path, 'Accidents.csv')))[,c(1:3,6:12,14,15,19), with=FALSE]
for(colname in names(Accident)) {
Accident[[colname]] <- as.numeric(Accident[[colname]])
}
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
new_log_reg_model <- train(
#  x=x_train[,1:5, with=FALSE],
x=new_x_train,
y=y_train,
preProcess=c('center', 'scale'),
method='plr', # Penalized Logistic Regression
metric=caret_optimized_metric,
trControl=caret_train_control,
tuneGrid=expand.grid(
lambda=0, # weight penalty parameter
cp='aic')) # complexity parameter (AIC / BIC)
new_log_reg_pred_probs <- predict(
new_log_reg_model, newdata=new_x_valid, type='prob')
new_log_reg_oos_performance <- bin_classif_eval(
new_log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
plot.new()
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
new_Accident <- as.data.table(read.csv(file.path(data_folder_path, 'Accidents.csv')))[,c(1:3,6:12,14,15,19), with=FALSE]
for(colname in names(new_Accident)) {
new_Accident[[colname]] <- as.numeric(new_Accident[[colname]])
}
new_x_train <- new_Accident[train_indices]
new_x_valid <- new_Accident[-train_indices]
new_log_reg_model <- train(
#  x=x_train[,1:5, with=FALSE],
x=new_x_train,
y=y_train,
preProcess=c('center', 'scale'),
method='plr', # Penalized Logistic Regression
metric=caret_optimized_metric,
trControl=caret_train_control,
tuneGrid=expand.grid(
lambda=0, # weight penalty parameter
cp='aic')) # complexity parameter (AIC / BIC)
new_log_reg_pred_probs <- predict(
new_log_reg_model, newdata=new_x_valid, type='prob')
new_log_reg_oos_performance <- bin_classif_eval(
new_log_reg_pred_probs$yes, y_valid, thresholds=prob_thresholds)
plot.new()
plot(x=1 - new_log_reg_oos_performance$specificity,
y=new_log_reg_oos_performance$sensitivity,
type = "l", col='darkgreen', lwd=3,
xlim = c(0., 1.), ylim = c(0., 1.),
main = "ROC Curves (Validation Data)",
xlab = "1 - Specificity", ylab = "Sensitivity")
abline(a=0,b=1,lty=2,col=8)
lines(x=1 - log_reg_oos_performance$specificity,
y=log_reg_oos_performance$sensitivity,
col='green', lwd=3)
legend('right', c('Original', 'With Alcohol'),
lty=1, col=c('darkgreen', 'green'), lwd=3, cex=1.)
source('C:/Users/msnrk/Dropbox/MBA Journey/1.Survive on MBA/03.Booth/Class/2015-4 Fall/41204 Machine Learning/Midterm/Midterm_Q1.R', echo=TRUE)
source('C:/Users/msnrk/Dropbox/MBA Journey/1.Survive on MBA/03.Booth/Class/2015-4 Fall/41204 Machine Learning/Midterm/Midterm_Q1.R', echo=TRUE)
source('C:/Users/msnrk/Dropbox/MBA Journey/1.Survive on MBA/03.Booth/Class/2015-4 Fall/41204 Machine Learning/Midterm/Midterm_Q1.R', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
source('~/.active-rstudio-document', echo=TRUE)
chooseCRANmirror(ind=1)
## nhl hockey analysis
## the data is in gamlr.
## You need to first install this,
## via install.packages("gamlr")
## Q1
library(gamlr) # loads Matrix as well
help(hockey) # describes the hockey data and shows an example regression
data(hockey) # load the data
# Combine the covariates all together
x <- cBind(config,team,player) # cBind binds together two sparse matrices
# build 'y': home vs away, binary response
y <- goal$homegoal
y_home_away <- y
y_home_away[y_home_away == 0] <- -1
n_goals_plus <- colSums((y_home_away > 0) * (player > 0) + (y_home_away < 0) * (player < 0))
n_goals_minus <- colSums((y_home_away < 0) * (player > 0) + (y_home_away > 0) * (player < 0))
# LASSO regression
nhlreg <- gamlr(x, y,
free=1:(ncol(config)+ncol(team)), ## free denotes unpenalized columns
family="binomial", standardize=FALSE)
# Just plotting first
plot(nhlreg)
# coefficients (grab only the players)
# AICc selection
Baicc <- coef(nhlreg)[colnames(player),]
# Order players by Betas
Baicc <- sort(Baicc, decreasing=TRUE)
# Pick Top 10 players based on betas
top10 <- head(Baicc,n=10)
top10_list <- cbind(1:10, top10, exp(top10), n_goals_plus[names(top10)], n_goals_minus[names(top10)])
colnames(top10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
# Pick Worst 10 players based on betas
worst10 <- tail(Baicc,n=20)
worst10_list <- cbind(1:20, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_min
us[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
# Pick Worst 10 players based on betas
worst10 <- tail(Baicc,n=20)
worst10_list <- cbind(1:20, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
print(worst10_list)
# Pick Worst 10 players based on betas
worst10 <- tail(Baicc,n=10)
worst10_list <- cbind(1:10, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
worst10 <- tail(Baicc,n=10)
worst10[,sort.list(worst10, decreasing = FALSE)]
worst10_list <- cbind(1:10, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
worst10 <- sort(worst10, decreasing = FALSE)
worst10 <- tail(Baicc,n=10)
worst10 <- sort(worst10, decreasing = FALSE)
worst10_list <- cbind(1:10, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
print(worst10_list)
names(_goals_plus)
names(n_goals_plus)
n_goals_plus("ERIC_BREWER")
n_goals_plus["ERIC_BREWER"]
n_goals_minus["ERIC_BREWER"]
biacc["ERIC_BREWER"]
Biacc["ERIC_BREWER"]
baicc["ERIC_BREWER"]
Baicc["ERIC_BREWER"]
worst10 <- tail(Baicc,n=20)
worst10 <- sort(worst10, decreasing = FALSE)
worst10_list <- cbind(1:20, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
print(worst10_list)
# Pick Worst 10 players based on betas
worst10 <- tail(Baicc,n=30)
worst10 <- sort(worst10, decreasing = FALSE)
worst10_list <- cbind(1:30, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
print(worst30_list)
print(worst10_list)
# Pick Worst 10 players based on betas
worst10 <- tail(Baicc,n=50)
worst10 <- sort(worst10, decreasing = FALSE)
worst10_list <- cbind(1:50, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
print(worst10_list)
# Pick Worst 10 players based on betas
worst10 <- tail(Baicc,n=100)
worst10 <- sort(worst10, decreasing = FALSE)
worst10_list <- cbind(1:100, worst10, exp(worst10), n_goals_plus[names(worst10)], n_goals_minus[names(worst10)])
colnames(worst10_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
print(worst10_list)
min(PM_classic)
source('C:/Users/msnrk/Google ドライブ/MBA Journey/1.Survive on MBA/03.Booth/Class/2016-1 Winter/41201 Big Data/Week 3/hockey_MK.R', echo=TRUE)
## Bonus
Pk <- exp(Baicc)/(1+exp(Baicc))
Gk <- colSums(abs(player)) # abs(onice) is 1 if any goal, 0 otherwise.
PM_ajust <- Gk*(2*Pk-1)
sort(PM_adjust, decreasing=TRUE)[1:10] # all studs
# calculate classic PM
PM_classic <- colSums(player*y_home_away)
names(PM_classic) <- colnames(player)
sort(PM_classic, decreasing=TRUE)[1:10]
# plot
plot(PM_classic, PM_adjust, main="Adjusted Plus-Minus vs. Classic Plus-Minus",
xlab="Classic Plus-Minus", ylab="Adjusted Plus-Minus", pch=20)
abline(a=0,b=1,col="blue", lty=2) # y=x line
#text(PM_classic,PM_ajust,labels=colnames(player)) # plot names
# above the line -> PM under-rates performance
# below the line -> PM over-rates performance
# based on the plot many players who are well-rated by PM are over-rated
## Bonus
Pk <- exp(Baicc)/(1+exp(Baicc))
Gk <- colSums(abs(player)) # abs(onice) is 1 if any goal, 0 otherwise.
PM_adjust <- Gk*(2*Pk-1)
sort(PM_adjust, decreasing=TRUE)[1:10] # all studs
# calculate classic PM
PM_classic <- colSums(player*y_home_away)
names(PM_classic) <- colnames(player)
sort(PM_classic, decreasing=TRUE)[1:10]
# plot
plot(PM_classic, PM_adjust, main="Adjusted Plus-Minus vs. Classic Plus-Minus",
xlab="Classic Plus-Minus", ylab="Adjusted Plus-Minus", pch=20)
abline(a=0,b=1,col="blue", lty=2) # y=x line
#text(PM_classic,PM_ajust,labels=colnames(player)) # plot names
# above the line -> PM under-rates performance
# below the line -> PM over-rates performance
# based on the plot many players who are well-rated by PM are over-rated
PM_adjust["ERIC_BREWER"]
player["ERIC_BREWER"]
player("ERIC_BREWER")
?'%*%'
?cv.gamlr
# Pick Top 10 players based on betas
top10_std <- head(Baicc_std,n=10)
top10_std_list <- cbind(1:10, top10_std, exp(top10_std), n_goals_plus[names(top10_std)], n_goals_minus[names(top10_std)])
colnames(top10_std_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
# Pick Worst 10 players based on betas
worst10_std <- tail(Baicc_std,n=10)
#worst10_std <- sort(worst10, decreasing = FALSE)
worst10_std_list <- cbind(1:10, worst10_std, exp(worst10_std), n_goals_plus[names(worst10_std)], n_goals_minus[names(worst10_std)])
colnames(worst10_std_list) <- cbind("Rank", "Beta", "Exp(Beta", "Goals_For", "Goals_Against")
worst10_std_list
devtools::install_github('rstudio/rsconnect')
library('devtools')
install.packages('devtools')
devtools::install_github('rstudio/rsconnect')
rsconnect::setAccountInfo(name='msnrkobayashi',
token='4E758925B9B0644F4CFC7279311DF342',
secret='hmV+QVgYX6R737koBfn9cUJXrb8qE2Ez1f+ac4Fu')
library(rsconnect)
rsconnect::deployApp('path/to/your/app')
install.packages("rCharts")
require(devtools)
install_github('rCharts', 'ramnathv')
r1 <- rPlot(mpg ~ wt | am + vs, data = mtcars, type = "point", color = "gear")
r1$print("chart1")
rPlot?
library('rChart')
cls = c("integer", "character", "integer")
mydata <- read.csv("activity.csv", head=TRUE, colClasses=cls, na.strings="NA")
setwd("C:/Users/msnrk/Github/coursera/datascience/Reproducible_Research/RepData_PeerAssessment1")
cls = c("integer", "character", "integer")
mydata <- read.csv("activity.csv", head=TRUE, colClasses=cls, na.strings="NA")
head(mydata)
dailysum <- tapply(mydata$steps, mydata$date, sum, na.rm=TRUE)
dailysum <- dailysum[!is.na(dailysum)]
hist(dailysum, col="green", breaks=10, xlab="Daily total steps",main="Distribution of daily total steps")
daily_sum
dailysum
hist(dailysum, col="green", breaks=20, xlab="Daily total steps",main="Distribution of daily total steps")
dailysum <- dailysum[!is.na(dailysum)]
hist(dailysum, col="green", breaks=20, xlab="Daily total steps",main="Distribution of daily total steps")
dailysum <- tapply(mydata$steps, mydata$date, sum, na.rm=TRUE, simplify=T)
dailysum <- dailysum[!is.na(dailysum)]
hist(dailysum, col="green", breaks=20, xlab="Daily total steps",main="Distribution of daily total steps")
mean(mydata$steps,na.rm=TRUE); median(mydata$steps, na.rm=TRUE)
mean(dailysum); median(dailysum)
int_ave <- tapply(mydata$steps, mydata$interval, mean, na.rm=TRUE)
df_int_ave <- data.frame(interval=as.integer(names(int_ave)), avg=int_ave)
plot(df_int_ave$interval, df_int_ave$ave, type="l",xlab="5-minute intervals",
ylab="average steps in the interval across all days")
step_max <- max(df_int_ave$ave)
df_int_ave[df_int_ave$ave==step_max,]
mydata$date <- as.Date(df$date)
cls = c("integer", "character", "integer")
mydata <- read.csv("activity.csv", head=TRUE, colClasses=cls, na.strings="NA")
head(mydata)
mydata$date <- as.Date(mydata$date)
mydata_ign <- subset(df, !is.na(mydata$steps))
mydata_ign <- subset(mydata, !is.na(mydata$steps))
dailysum <- tapply(mydata_ign$steps, mydata_ign$date, sum, na.rm=TRUE, simplify=T)
dailysum <- dailysum[!is.na(dailysum)]
hist(dailysum, col="green", breaks=20, xlab="Daily total steps",main="Distribution of daily total steps")
mean(dailysum); median(dailysum)
int_ave <- tapply(mydata2$steps, mydata2$interval, mean, na.rm=TRUE)
mydata2 <- subset(mydata, !is.na(mydata$steps))
dailysum <- tapply(mydata2$steps, mydata2$date, sum, na.rm=TRUE, simplify=T)
dailysum <- dailysum[!is.na(dailysum)]
hist(dailysum, col="green", breaks=20, xlab="Daily total steps",main="Distribution of daily total steps")
int_ave <- tapply(mydata2$steps, mydata2$interval, mean, na.rm=TRUE)
df_int_ave <- data.frame(interval=as.integer(names(int_ave)), avg=int_ave)
plot(df_int_ave$interval, df_int_ave$ave, type="l",xlab="5-minute intervals",
ylab="average steps in the interval across all days")
mean(dailysum); median(dailysum)
plot(df_int_ave$interval, df_int_ave$ave, type="l",xlab="5-minute intervals",
ylab="average steps in the interval across all days" xlim=c(0,2000))
plot(df_int_ave$interval, df_int_ave$ave, type="l",xlab="5-minute intervals",
ylab="average steps in the interval across all days" ,xlim=c(0,2000))
head(int_ave)
tail(int_ave)
df_int_ave <- data.frame(interval=as.integer(names(int_ave)), ave=int_ave)
plot(df_int_ave$interval, df_int_ave$ave, type="l",xlab="5-minute intervals",
ylab="average steps in the interval across all days")
step_max <- max(df_int_ave$ave)
df_int_ave[df_int_ave$ave==step_max,]
step_max
sum(is.na(mydata$steps))
mydata3 <- mydata
ndx <- is.na(mydata3$steps)
ndx
int_ave <- tapply(mydata2$steps, mydata2$interval, mean, na.rm=TRUE, simplify=T)
mydata2$steps[ndx] <- int_ave[as.character(mydata2$interval[ndx])]
mydata3$steps[ndx] <- int_ave[as.character(mydata2$interval[ndx])]
mydata3$steps[ndx] <- int_ave[as.character(mydata3$interval[ndx])]
dailysum2 <- tapply(mydata3$steps, mydata3$date, sum, na.rm=TRUE, simplify=T)
hist(x=dailysum2, col="green", breaks=10, xlab="daily steps",
main="Distribution of daily total steps (with missing data imputed)")
mean(dailysum2); median(dailysum2)
is_weekday <- function(d) {
wd <- weekdays(d)
ifelse (wd == "Saturday" | wd == "Sunday", "weekend", "weekday")
}
wx <- sapply(df_impute$date, is_weekday)
wx <- sapply(mydata3$date, is_weekday)
mydata3$wk <- as.factor(wx)
head(mydata3)
head(mydata3,30)
head(mydata3,100)
head(mydata3,100:200)
wk_df <- aggregate(steps ~ wk+interval, data=mydata3, FUN=mean)
library(lattice)
xyplot(steps ~ interval | factor(wk),
layout = c(1, 2),
xlab="Interval",
ylab="Number of steps",
type="l",
lty=1,
data=wk_df)
mmydata
mydata3
sum(mydata3$wk=="weekend")
weekdays(mydata3$date)
